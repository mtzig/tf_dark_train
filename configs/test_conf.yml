# Model and Tokenizer Configuration
model_name: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
max_seq_length: 1024
seed: 4234
data_seed: 505

# Dataset Configuration
dataset_path: "./data/tfd_small.pkl"
# dataset_path: "./debug_chat_traces.pkl"


# Validation Configuration
num_validation_samples: 5
per_device_eval_batch_size: 2
eval_steps: 2

# LoRA (Low-Rank Adaptation) Configuration
lora_r: 8
lora_alpha: 16
lora_dropout: 0.1
lora_target_modules:
  - "q_proj"
  - "k_proj"
  - "v_proj"
  - "o_proj"

# Training Arguments
output_dir: "tinyllama-1.1b-sft-test"
num_train_epochs: 1
per_device_train_batch_size: 2
gradient_accumulation_steps: 2
learning_rate: 3.0e-4
lr_scheduler_type: "cosine"
warmup_ratio: 0.03
weight_decay: 0.001
optim: "paged_adamw_32bit"
bf16: true
fp16: false
logging_strategy: "steps"
logging_steps: 5
save_strategy: "steps"
save_steps: 20
save_total_limit: 2

# Hugging Face Hub and W&B Configuration
push_to_hub: true # Disable for quick tests
# hf_hub_repo_id: "your-hf-username/tinyllama-1.1b-sft-lora" # CHANGE THIS to your desired Hub repo ID
report_to: "wandb"
wandb_project: "tinyllama-finetune-test"
