# Model and Tokenizer Configuration
model_name: "Qwen/Qwen3-8B" # Specify the Qwen model you want to fine-tune
max_seq_length: 1024 # Maximum sequence length for the model
seed: 4234
data_seed: 505

# Dataset Configuration
dataset_path: "chat_traces.pkl" # Path to your pickle file with chat data

# Validation Configuration
num_validation_samples: 16
per_device_eval_batch_size: 16
eval_steps: 2

# LoRA (Low-Rank Adaptation) Configuration
lora_r: 16                # The rank of the update matrices. Lower rank means fewer parameters to train.
lora_alpha: 32            # LoRA scaling factor.
lora_dropout: 0.05        # Dropout probability for LoRA layers.
lora_target_modules:      # Modules to apply LoRA to. These are common targets for Qwen2 models.
  - "q_proj"
  - "k_proj"
  - "v_proj"
  - "o_proj"
  - "gate_proj"
  - "up_proj"
  - "down_proj"

# Training Arguments
output_dir: "qwen3-8b-tfdark-lora" # Directory to save checkpoints and final model
num_train_epochs: 1
per_device_train_batch_size: 16
gradient_accumulation_steps: 4 # Effective batch size = batch_size * grad_accumulation
learning_rate: 2.0e-4
lr_scheduler_type: "cosine"
warmup_ratio: 0.03
weight_decay: 0.001
optim: "paged_adamw_32bit" # Paged AdamW optimizer for memory efficiency
bf16: true # Use bfloat16 for training on A100/H100 GPUs. Set to false if not supported.
fp16: false # Set to true for mixed-precision training on other GPUs.
logging_strategy: "steps"
logging_steps: 10
save_strategy: "steps"
save_steps: 50
save_total_limit: 2 # Only keep the last 2 checkpoints

# Hugging Face Hub and W&B Configuration
push_to_hub: true # Set to true to automatically push the final model to the Hub
# hf_hub_repo_id: "your-hf-username/qwen2-7b-sft-lora" # CHANGE THIS to your desired Hub repo ID
report_to: "wandb" # Enable Weights & Biases logging
wandb_project: "qwen3-8b-tfdark" # Your project name in W&B